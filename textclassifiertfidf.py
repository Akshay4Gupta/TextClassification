# -*- coding: utf-8 -*-
"""TextClassifiertfidf.ipynb

d f means feature engineering
d f f2 means SelectPercentile

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CA16IJ9kl81h8WV3aHWzYWJmoYNd3WuI
"""

#from google.colab import drive
#drive.mount('/content/drive')

import pandas as pd
import math
import time
import re
import nltk
import sys
import random
import pickle
import numpy as np
import multiprocessing as mp
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.tokenize import TweetTokenizer
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import GaussianNB
from sklearn.feature_selection import SelectPercentile, chi2
from tqdm import tqdm
from multiprocessing import Process

# nltk.download('stopwords')

trainingData = pd.read_csv("drive/My Drive/ML/TextClassification/training.1600000.processed.noemoticon.csv",
header = None,
names = ['polarity', 'id', 'date', 'query',
'username', 'tweet'],
encoding = "Latin-1")

testingData = pd.read_csv("drive/My Drive/ML/TextClassification/testdata.manual.2009.06.14.csv",
header = None,
names = ['polarity', 'id', 'date', 'query',
'username', 'tweet'],
encoding = "ISO-8859-1")

stop_words = set(stopwords.words('english')).union({',', '.', '!', ' ', '\n', '\t'})
stemming = PorterStemmer()
twittere = re.compile("^@[a-zA-Z0-9_]*$")
clf = GaussianNB()

def tokenize(word):
  wordlist = word.replace(',', ' ').replace('.', ' ').split()
  word_no_twitter = [w for w in wordlist if (not twittere.match(w))]
  return word_no_twitter

def accuracy(Data, philyEquals, phi, keysInDict, wcyEquals, confusionMatrix, case):
    GivenxyEqual = list()
    correct = 0
    wrong = 0
    LISTOFPROB0 = []
    LISTOFPROB1 = []

    for (id, (polarity, words)) in Data.items():
        gxye0 = phi[0]
        gxye1 = phi[1]
        for k in words:
            if k in philyEquals[0]:
                gxye0 += math.log(philyEquals[0][k])
            else:
                gxye0 += math.log(1/(keysInDict + wcyEquals[0]))
            if k in philyEquals[1]:
                gxye1 += math.log(philyEquals[1][k])
            else:
                gxye1 += math.log(1/(keysInDict + wcyEquals[1]))
        GivenxyEqual.append([math.log(phi[0]) + gxye0, math.log(phi[1]) + gxye1])
        if(GivenxyEqual[-1][0] > GivenxyEqual[-1][1]):
            if(polarity == 0):
                correct += 1
                confusionMatrix[0][0] += 1
            else:
                wrong += 1
                confusionMatrix[0][1] += 1

        if(GivenxyEqual[-1][0] < GivenxyEqual[-1][1]):
            if(polarity == 0):
                wrong += 1
                confusionMatrix[1][0] += 1
            else:
                correct += 1
                confusionMatrix[1][1] += 1
        LISTOFPROB0.append(gxye0)
        LISTOFPROB1.append(gxye1)
    return (correct, wrong, confusionMatrix, LISTOFPROB0, LISTOFPROB1)

# nltk.download('stopwords')
def wordsExtraction(data, caseing):
  dataBuilt = dict()
  sentence = ''
  for j in data.itertuples():
    if(j[1] != 0 and j[1] != 4):
      continue
    words = list()
    if 'd' in caseing:
      word_tokens_noTwitter = tokenize(j[6])

      # ----------------------------------------------------------twitter tokenize-------------------------------------------------------
      # noTwitter_tokens = tknzr.tokenize(j[6])
      # word_tokens_noTwitter = []
      # for w in noTwitter_tokens:
      #     word_tokens_noTwitter += w.replace(',', ' ').replace('.', ' ').split()          #first part Q1(a)
      # ----------------------------------------------------------twitter tokenize-----------------------------------------------------------

      words = [stemming.stem(w) for w in word_tokens_noTwitter if not w in stop_words]
    elif 'a' in caseing:
      words = j[6].replace(',', ' ').replace('.', ' ').split()
    sentence = ' '.join(words)
    dataBuilt[j[2]] = (j[1], words, sentence)
  return dataBuilt

caseing2 = input("caseing = ['d', 'f', 'f2']: ")


trainingDataPreprocessed = wordsExtraction(trainingData,caseing)
allDocuments = []
for (id, (polarity, words, sentence)) in trainingDataPreprocessed.items():
  allDocuments.append((sentence, polarity))
Z = list(zip(*allDocuments))
X = Z[0]
Y = Z[1]

testingDataPreprocessed = wordsExtraction(testingData, caseing)
testingDocuments = []
for (id, (polarity, words, sentence)) in testingDataPreprocessed.items():
  testingDocuments.append((sentence, polarity))
z = list(zip(*testingDocuments))
x = list(z[0])
y = list(z[1])


def main(c):
  caseing = caseing2.split()
  vectorizer = TfidfVectorizer(min_df = c)
  X_new = vectorizer.fit_transform(X)
  SPX = SelectPercentile(chi2, percentile=10).fit(X_new, Y)
  if 'f2' in caseing:
    X_new =  SPX.transform(X_new)

  clas = np.unique(Y)
  step = 2500
  for i in range(0, X_new.shape[0], step):
    clf.partial_fit(X_new[i:(i+step)].toarray(), Y[i:i+step], clas)


  X1_vt = vectorizer.transform(x)
  if 'f2' in caseing:
    X1_vt = SPX.transform(vectorizer.transform(x))

  print("accuracy over test tfidf: ", clf.score(X1_vt.toarray(), y))
  return((clf.predict_log_proba(X1_vt.toarray()), clf.predict_proba(X1_vt.toarray())))


def roc(LISTOFPROB):
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    data = np.array(testingData)[:,0].reshape(-1,1).astype(int)
    data0 = data[data!=2]
    for i in range(2):
    	fpr[i], tpr[i], _ = roc_curve(data0,LISTOFPROB[i], pos_label = 4)
    	roc_auc[i] = auc(fpr[i], tpr[i])

    # Compute micro-average ROC curve and ROC area
    # fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_score.ravel())
    # roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
    plt.figure()
    lw = 2
    plt.plot(fpr[0], tpr[0], color='darkorange',
         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[0])
    # plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
    # plt.xlim([0.0, 1.0])
    # plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC')
    plt.legend(loc="lower right")
    plt.show()

C = [5e-4, 6e-4, 7e-4]
from multiprocessing import Pool
p = Pool(3)
x = [(c) for c in C]
result = p.map(main, x)
if 'doit' in caseing:
    for l in result:
        roc(l[0])

# l = []
# P = [Process(target=main, args=(c, caseing)) for c in C]
# for p in P:
#   p.start()
# for p in P:
#   p.join()
"""# accuracy over test tfidf:  0.5431754874651811"""

# np.zeros(10**9, dtype = 'float128')
